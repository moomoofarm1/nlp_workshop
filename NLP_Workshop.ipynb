{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TlOmTfmIQPMK"
      },
      "source": [
        "# SICSS-IAS 2023: Computational Text Analysis\n",
        "\n",
        "> Hendrik Erz, IAS | <hendrik.erz@liu.se> | Twitter: @sahiralsaid\n",
        "> \n",
        "> MaÃ«l Lecoursonnais, IAS | <mael.lecoursonnais@liu.se> | Twitter: @M_Lecoursonnais\n",
        "\n",
        "Welcome to the practical part of the Computational Text Analaysis day! In this notebook, you will try out some of the methods covered in the morning lecture. In particular, the methods covered will be:\n",
        "\n",
        "* tf-idf scores\n",
        "* topic modeling\n",
        "* Word2Vec\n",
        "\n",
        "Below, you will see several exercises that cover most of the steps from an unprocessed text corpus to a final trained model. As a default choice, we provide a corpus of the [**U.S. State of the Union (SOTU)**](https://en.wikipedia.org/wiki/State_of_the_Union) speeches, but you are welcome to use your own corpus. The code is agnostic to the data used and should work with everything.\n",
        "\n",
        "You will work on these examples in smaller groups with guidance from us.\n",
        "\n",
        "## I want to use my own corpus!\n",
        "\n",
        "If you want to use your own corpus, that's great! However, it should fulfill certain criteria so that the methods make sense. If your corpus does not fulfill these criteria, some or all of these methods may still apply, but additional tweaks to the parameters may be necessary. **If you are uncertain, just ask! :)**\n",
        "\n",
        "* The corpus should at least contain 100 documents (and probably not more than 10,000)\n",
        "* Tweets are probably too short. Aim for about 300-1,200 words per document. (A few too short or too long documents don't matter, as long as most of them fall in the range.)\n",
        "* The corpus should be in a language that uses individual characters that are combined to form words (read: western scripts), so scripts such as Chinese, Korean, or Japanese may not work with these methods\n",
        "* The corpus should be formed from a single source (i.e. same language, same type of document, etc.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries: Installing the necessary packages\n",
        "\n",
        "This workbook relies on a set of packages. Depending on what package manager you use, there are various ways to install them (e.g., with Conda, pip, or virtualenv).\n",
        "\n",
        "Make sure to install them according to how you have set up Python. If you use plain `pip`, here is how you can install them:\n",
        "\n",
        "```bash\n",
        "python -m pip install pandas     # For creating DataFrames\n",
        "python -m pip install numpy      # For numerical operations, specifically matrices\n",
        "python -m pip install gensim     # Used for the word2vec model\n",
        "python -m pip install sklearn    # Contains the tf-idf and Latent Dirichlet Allocation models\n",
        "python -m pip install nltk       # Offers word- and sentence tokenizers as well as stopword lists\n",
        "python -m pip install matplotlib # For plotting, similar to ggplot in R\n",
        "```\n",
        "\n",
        "**Since these will come in handy, here is also a list of all manuals for the respective packages:**\n",
        "\n",
        "* [pandas](https://pandas.pydata.org/docs/)\n",
        "* [numpy](https://numpy.org/doc/1.24/reference/index.html)\n",
        "* [gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
        "* [sklearn](https://scikit-learn.org/stable/user_guide.html)\n",
        "* [nltk](https://www.nltk.org/)\n",
        "* [matplotlib](https://matplotlib.org/stable/users/index)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8bOzIeukQxKZ"
      },
      "source": [
        "## Loading the Corpus\n",
        "\n",
        "The first step you'll have to do is load the corpus. How you are loading the corpus heavily depends on the data format in which it is stored. The demo-corpus is stored as a TSV-file (tab-separated values). TSV-files are a very common way to store textual data, since tab-characters have no linguistic meaning, and therefore can be used as separators of the fields.\n",
        "\n",
        "Your task in this exercise is to write a function that will return parts of the corpus in a way that the models we are using works with.\n",
        "\n",
        "***\n",
        "\n",
        "We will use a [**generator**](https://wiki.python.org/moin/Generators) to load our corpus, since a generator helps us keep the memory footprint small and therefore to keep the model training times low. For our small corpus, this may seem over-engineered, but most corpora are too large to be loaded at once, so it is important to learn this technique early.\n",
        "\n",
        "> If you want to learn more about why generators are useful to us as social scientists, [click here](https://www.hendrik-erz.de/post/what-is-a-generator).\n",
        "\n",
        "Your function should `yield` the speeches one after another:\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "    ('Speech Title', 'Author name', 'date', 'This is the first speech'),\n",
        "    ('Speech Title', 'Second author', 'date', 'This is a second speech'),\n",
        "    # ...\n",
        "]\n",
        "```\n",
        "\n",
        "The corpus contains additional metadata that you can use if you wish to explore it further. The additional data should be returned here so that you have it available if you need it.\n",
        "\n",
        "**Whenever you need the speeches, just call `speeches()` in your code. The speech text is accessible from each tuple at the third index (`speech[3]`).**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Load the Corpus\n",
        "\n",
        "Below, write a function that reads in the corpus. Remember to use a generator (`yield` instead of `return`) and to return tuples of `(title, author, date, text)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aogeN5TsSNZR"
      },
      "outputs": [],
      "source": [
        "def speeches ():\n",
        "  \"\"\"A generator that yields (title, author, date, text) tuples\"\"\"\n",
        "  # NOTE: Ensure that the file `sotu.tsv` is in the same directory as this notebook.\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ9cSO93VppW",
        "outputId": "bbfa1032-1d83-4ece-edf9-75d59a8f749e"
      },
      "outputs": [],
      "source": [
        "# Run this cell to ensure your function works properly and returns 251 speeches.\n",
        "assert sum([1 for x in speeches()]) == 251, \"Wrong number of speeches!\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting to Know the Corpus\n",
        "\n",
        "Now that we have access to the text in the corpus, we should make sure we have some bearings in the corpus. For this, a very simple method is to simply start counting words. This is not necessarily the most informative method, but it will give us some data that we can use to make some sense of the corpus. Additionally, it is extremely fast and therefore cheap to do.\n",
        "\n",
        "Getting to some word counts involves just two steps:\n",
        "\n",
        "1. First, we need a **tokenizer** that can split up a speech into tokens.\n",
        "2. With speeches tokenized into individual words, we can count those."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9zLHzCvEZu"
      },
      "source": [
        "### Exercise 2: Write a text tokenizer\n",
        "\n",
        "Below, write a function that takes a single speech and returns a list of tokens within this speech. You should remove digits, punctuation marks, and other symbols that do not comprise regular, English words. Also, ensure that capitalization of letters does not matter by transforming the speech into lower case first.\n",
        "\n",
        "> TIP: The NLTK package offers a lot of useful functions for working with natural language. It includes functions to remove stopwords and to tokenize a text. Also, the String class of Python provides additional easy functions you can use. Have a look at the documentation for [**nltk.corpus**](https://www.nltk.org/api/nltk.corpus.html) and [**nltk.tokenize**](https://www.nltk.org/api/nltk.tokenize.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmpNEFrJTbqk"
      },
      "outputs": [],
      "source": [
        "# TODO: You will need to import functions from NLTK here.\n",
        "\n",
        "def tokenize_speech(speech):\n",
        "  \"\"\"Takes a single speech (string of text) and returns a list of all proper words contained within this speech.\"\"\"\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYkyW1DfXna4",
        "outputId": "072e18e4-3aaa-4f68-fe9e-f04f4c802686"
      },
      "outputs": [],
      "source": [
        "# Run this cell to see how the preprocessor processes the first speech.\n",
        "first_speech = next(speeches())\n",
        "tokenize_speech(first_speech[3])[0:10] # Look at the first ten words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Count words\n",
        "\n",
        "Now that we have a way to tokenize our speeches, we can count the words inside our corpus.\n",
        "\n",
        "Counting words in Python is very easy, since it has a useful function for us: [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter).\n",
        "\n",
        "> Below, write a function that counts the word frequencies of the entire corpus and returns a list of elements. Hint: The Counter class provides a handy method called `most_common`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_words ():\n",
        "  \"\"\"Returns a list of tuples that contain the words in the corpus and their frequency.\"\"\"\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let us now look at the ten most common, and then ten least common words:\n",
        "\n",
        "words = count_words()\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(words[0:10])\n",
        "\n",
        "print(\"\")\n",
        "print(\"Least common words:\")\n",
        "print(words[-10:])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Zipf's Law\n",
        "\n",
        "One great way of ensuring that your corpus does not include oddities and spurious data, a very simple verification involves checking for Zipf's law: The fact that a few words are very common, and many words are relatively uncommon. A histogram of words should resemple a logarithmic function.\n",
        "\n",
        "Below, plot the frequencies ordered by rank.\n",
        "\n",
        "> Note: If you are stuck, the matplotlib documentation has got you covered. Just make sure to plot the word frequencies against their rank, which involves sorting the list. Remember to set the y-axis scale to logarithmic and provide proper labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Write the plotting code here."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rk7RST298"
      },
      "source": [
        "## Computing tf-idf scores\n",
        "\n",
        "Word frequency counts are simple and straightforward, but do not say anything about the importance of words. The most common word of the English language, \"the\", will therefore always show up at the top of the distribution, but the word is not really informative. An improvement over simple word counts is tf-idf, or term-frequency, inverse document-frequency, which penalizes words that occur often across all documents, but increases the score for rare words that occur frequently in only some documents.\n",
        "\n",
        "Calculating tf-idf scores can be done \"manually\" as this only involves re-weighting the terms, but for practical usage, there are some libraries that already do that for you.\n",
        "\n",
        "Calculating tf-idf scores consists of three steps:\n",
        "\n",
        "1. Calculate the term frequencies of words within each document\n",
        "2. Calculate the inverse document frequency for each word\n",
        "3. For each word in each document, calculate tf-idf by multiplying the document-specific frequency with the word's IDF-score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVDE8UhMy_SY"
      },
      "source": [
        "### Exercise 4: Build a Vocabulary\n",
        "\n",
        "For tf-idf scores, it makes sense to manually build a vocabulary beforehand. While calculating tf-idf scores does not require a vocabulary, it gives us control over what words should even be included in the calculation, and makes it easier to analyze which words received which scores. Additionally, we will need a vocabulary for our next method, LDA topic models, so it makes sense to create one here.\n",
        "\n",
        "Creating a manual vocabulary has a few advantages over letting the packages handle it: first, you can easily filter it before providing it to text analysis models, ensuring that you only use words you want to. Second, you can inspect it to detect potential issues in your corpus (e.g., hard to detect but frequent typos that may hamper the analyses). Third, it makes the process of using the vocabulary transparent.\n",
        "\n",
        "The easiest form of a vocabulary is a dictionary that maps words to indices:\n",
        "\n",
        "```python\n",
        "vocab = {\n",
        "    'word': 0,\n",
        "    'word2': 1,\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "Since we also need to figure out words by their indices after running some of our methods, we should create a so-called `i2w`-dictionary. The `i2w` (\"index to word\") performs the reverse lookup and maps indices to words:\n",
        "\n",
        "```python\n",
        "iw2 = {\n",
        "    0: 'word',\n",
        "    1: 'word2',\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "> Nerd fact: If you are curious why we would want to create two dictionaries: the main reason for this is performance. Dictionaries in Python are implemented as [hash tables](https://en.wikipedia.org/wiki/Hash_table), so looking up indices and words is almost instantaneous, which makes this method much faster than ordinary lists.\n",
        "\n",
        "We can then use these dictionaries to retrieve indices or words, depending on what we need:\n",
        "\n",
        "```python\n",
        "idx = vocab['president']\n",
        "# > May yield '24'\n",
        "\n",
        "word = i2w[24]\n",
        "# > May yield 'president'\n",
        "```\n",
        "\n",
        "Below, write a function that returns both a vocab and an i2w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYkvFH6Fzlkd"
      },
      "outputs": [],
      "source": [
        "def build_vocab ():\n",
        "  vocab = {}\n",
        "  i2w = {}\n",
        "  # First generate the word -> index mapping\n",
        "  # Then reverse it, i.e. index -> word mapping.\n",
        "  return vocab, i2w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to create the vocabulary\n",
        "vocab, i2w = build_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let us have a first look at the first ten words in our vocabulary. Note how\n",
        "# the first ten words in our vocab equal the first words of the first speech\n",
        "# (sans stopwords, numbers, etc).\n",
        "list(vocab.keys())[0:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaNUs1KeK9B"
      },
      "source": [
        "### Exercise 5: Calculate tf-idf scores\n",
        "\n",
        "Now that we have a vocabulary, we can calculate the tf-idf scores. Below, write a function that calculates tf-idf scores for our corpus and then outputs the most relevant word for each speech.\n",
        "\n",
        "> The library `scikit-learn` has a useful function for that called `TfidfVectorizer`. You will need to pass it the vocabulary and the speeches. Note additionally that the `TfidfVectorizer` will by default normalize the scores, so ther values will be different (but they will have the same ordering).\n",
        "\n",
        "The data will be returned in the following form, which is, as you can see, a document-term matrix where each row corresponds to a document, and each column corresponds to a word. The cell contents contain the tf-idf score for the given word in the given document. This is why we have created a vocabulary and an `i2w` beforehand: This way you can map the column-index back to the proper words. In other words: If the most important word for a document is at index 23, we can call `i2w[23]` to find out the corresponding word.\n",
        "\n",
        "```python\n",
        "tfidf = [\n",
        "  # Document 1\n",
        "  [\n",
        "    0.23,\n",
        "    0.11,\n",
        "    # ...\n",
        "  ],\n",
        "  # Document 2\n",
        "  [\n",
        "    0.412,\n",
        "    0.0012,\n",
        "    # ...\n",
        "  ],\n",
        "  # ...\n",
        "]\n",
        "```\n",
        "\n",
        "As an additional hint: The models that scikit-learn provides all share a common format that is intended to be used as such:\n",
        "\n",
        "1. Instantiate a new model, providing your settings for the model\n",
        "2. Provide data and `fit` the model to the data\n",
        "3. `transform` a set of data using the fitted model, which will return a document-term matrix (DTM) where each column corresponds to a term, and each row to a document. The cell then contains the actual scores.\n",
        "\n",
        "> Note that the function `fit_transform` performs steps 2 and 3 at the same time, which is sufficient for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlf364UDeaao"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tf_idf ():\n",
        "  \"\"\"This function calculates tf-idf scores, providing the vocabulary and our\n",
        "  tokenizer, and returns a document-term-matrix\"\"\"\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoJk1kg8f17T"
      },
      "outputs": [],
      "source": [
        "# Call the function to calculate the tf-idf scores\n",
        "tf_idf = calculate_tf_idf()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTqkGzUfWCf"
      },
      "source": [
        "### Exercise 6: Analysis of tf-idf scores\n",
        "\n",
        "Now that we have calculated tf-idf scores, it is time to have a look at them. Since we have already transformed them into an easily digestible format, this step code should be relatively simple. Below, write code to print out the highest scoring word for each speech, as well as, afterwards, the lowest-scoring word.\n",
        "\n",
        "> In order to do so, you need to iterate over all documents, and for each document find the word that has the highest and the lowest tf-idf score. An easy way to do so is to take an entire row (i.e. all terms) and utilize the functions [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) and [`np.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html), which return not the `max` or `min` _values_ of them (the tf-idf scores), but rather the _index_ at which the max/min values are. Then you can look up the word that corresponds to that index with the `i2w` score.\n",
        "\n",
        "Explain what makes the words important or unimportant, and what this means in the context of the SOTU corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN09mxipgBBV",
        "outputId": "c46d5f89-99f8-44cf-f3f3-b5e048b3dac0"
      },
      "outputs": [],
      "source": [
        "print(\"Most important words (according to tf-idf):\")\n",
        "# TODO: Print out most important words\n",
        "\n",
        "print(\"\")\n",
        "print(\"Least important words (according to tf-idf):\")\n",
        "# TODO: Print out least important words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f4LDxhirgDjr"
      },
      "source": [
        "## Running a Topic Model\n",
        "\n",
        "At this point we have already a certain idea about what is contained in our corpus: We know what the most and least frequent words are, and what the most and least important words according to tf-idf scores are.\n",
        "\n",
        "The next step to explore our corpus is to run a topic model. The most common model is Latent Dirichlet Allocation (LDA). The library `sklearn` already provides such a model. As mentioned above, running an LDA model works exactly as running the `TfidfVectorizer`. The main difference is what happens under the hood, and that you have to provide different arguments.\n",
        "\n",
        "The `TfidfVectorizer` was very happy with us simply providing a list of words. Running an LDA model requires a Document-Term Matrix (DTM) instead. Remember that in it, documents are defined as \"one hot\"-vectors. The matrix has the shape `(number of documents x number of words)`, and each cell is set to `0` if the document does not contain the word, and `1` if it does.\n",
        "\n",
        "With the `tokenize_speech()` function from above, we already have a function that splits speeches into tokens. Additionally, we have a vocabulary at hand. Building a DTM is simple with this:\n",
        "\n",
        "1. Build the vocabulary (here it can make sense to remove the most and least often occurting terms, but for the demo-corpus this is not absolutely necessary)\n",
        "2. Create an empty DTM that contains as many rows as there are documents, and as many columns as there are words in the vocabulary.\n",
        "3. Go over the documents and set the corresponding cells in the corresponding matrix row to `1`, if the given document contains a word in the vocabulary."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fTtUrW9iyM6U"
      },
      "source": [
        "### Exercise 7: Build a DTM\n",
        "\n",
        "Below, write a function that creates a DTM as outlined above.\n",
        "\n",
        "> Hint: In order to create an empty matrix, the numpy library already provides a handy function for that, called [`np.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html). Please note that you need to provide the argument `dtype=np.longlong` to ensure the model does not complain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXhoT2Jeh3FJ"
      },
      "outputs": [],
      "source": [
        "def build_dtm ():\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iOxl2vJJikKL"
      },
      "source": [
        "### Exercise 8: Run the Topic model\n",
        "\n",
        "Below, write a function that trains a topic model. We have already added the correct function import for you. One thing that you will need to do, however, is figure out three hyperparameters: K, alpha, and beta.\n",
        "\n",
        "Since we are dealing with a small corpus, let us just set $K = 10$. However, you still need to figure out a good alpha and a good beta. Beta should normally be larger than alpha, and both should be smaller than 0.5. Feel free to run the model several times while doing exercises 8 and 9 to figure out good values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m42pLJa7is6A"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def fit_lda_model ():\n",
        "  # First, retrieve the DTM and present the hyperparameters for the model.\n",
        "  dtm = build_dtm()\n",
        "  K = 10\n",
        "  alpha = 0.0 # TODO: Adapt\n",
        "  beta = 0.0 # TODO: Adapt\n",
        "\n",
        "  # Then instantiate the model, and fit it to our data\n",
        "  model = LatentDirichletAllocation(\n",
        "    n_components=K,\n",
        "    doc_topic_prior=alpha,\n",
        "    topic_word_prior=beta\n",
        "  )\n",
        "  model.fit(dtm)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvRyMb9Dus1R"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "model = fit_lda_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P9RRDWX5jPHU"
      },
      "source": [
        "### Exercise 9: Analyze the topic model\n",
        "\n",
        "The last step in this exercise is to analyse the topic model. The most common method is to simply output the most important words (here, let us use 10 words) for each topic and see if you can make out any semantic topics.\n",
        "\n",
        "Retrieving the most and least important words of the LDA matrix works analogously to how you did this with regard to the tf-idf counts. However, there are two important differences between tfidf and LDA:\n",
        "\n",
        "1. The matrix is not a document-term matrix, but a topic-term matrix, i.e. the rows contain topics, not documents\n",
        "2. In order to analyze topics, you'll need more than just the single most important word\n",
        "\n",
        "The topic-term-matrix is accessible with the property `components_` of the trained model. The shape of this matrix is `(n_topics, n_words)`.\n",
        "\n",
        "> TIP: In order to sort the words correctly, you can use the function [`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) (to retrieve a list of the word-indices, ordered by importance). Note however, that `argsort` sorts the words ascending, so the most important ones are the last in the list.\n",
        "\n",
        "Below, write a function that prints the ten most important words for each topic. Try to think of fitting topic labels. What topic do the words seem to focus on?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3szwevOvj8Qy"
      },
      "outputs": [],
      "source": [
        "def print_top_words (model):\n",
        "  # Since we don't want to output numbers, we need an index->word mapping\n",
        "  _, i2w = build_vocab()\n",
        "\n",
        "  # How many words do we want to output?\n",
        "  L = 10\n",
        "\n",
        "  raise NotImplementedError(\"Write your code here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZCTr_qCuJ6M",
        "outputId": "10f6456e-24f3-466b-f65d-ef25c7a32ed9"
      },
      "outputs": [],
      "source": [
        "# Call the function\n",
        "print_top_words(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2FqixBzziY"
      },
      "source": [
        "## Running Word2Vec\n",
        "\n",
        "The most advanced NLP method we will cover today is a Word2Vec model. Such a model encodes co-occurrence patterns of words in so-called word embeddings, vectors of numbers with 50, 100, 200, or 300 dimensions.\n",
        "\n",
        "Here, you will write the least code since we will be using the gensim-library to run Word2Vec. Despite it being the (mathematically) most complex model, it can be run very fast and efficiently.\n",
        "\n",
        "Additionally, while all the methods we have looked at so far are based on the assumption of a **bag of words** (BOW), word embeddings have a stricter assumption that words need to co-occur. Finally, `word2vec` **does not use a document-term-matrix**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wF-qkFnL0vOY"
      },
      "source": [
        "### Exercise 10: Run Word2Vec\n",
        "\n",
        "Below, write code that uses gensim's Word2Vec model and run it on our corpus.\n",
        "\n",
        "Train two models, one with a `window size` of 5, and one with 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF__0S4E0nYF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# TODO: Write your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oN9wQDhC1mtr"
      },
      "source": [
        "### Exercise 11: Analyze the word embeddings\n",
        "\n",
        "As a last exercise for today, here we analyze the word embeddings. Word embeddings are very good to find out what words are related to others. On gensim's model, you can check so by utilizing `model.wv.most_similar('word')`.\n",
        "\n",
        "> Below, print out the most similar words for `america`, `government`, `bank`, and `war` for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLM9FcP715SG",
        "outputId": "93d2b349-6414-4c20-a671-c94effbec35a"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This concludes today's introduction into text analysis methods. Notice how we produced almost no plots: These methods are all great for getting a bearing on some corpus. Once you have found some interesting patterns, it is time to throw the results of one or more of the above methods into a model and test hypotheses with it.\n",
        "\n",
        "One thing that is frequently done, for example, is to use the tfidf-scores as independent variables for a regression to see what words predict some metadata. You should pay attention to the fact that words are not really independent from each other, however.\n",
        "\n",
        "Regardless of whether you have used the provided corpus or your own: Try to come up with some research questions over the afternoon that you could check. Maybe this leads to a great project idea?\n",
        "\n",
        "If you have further questions regarding these methods, do not hesitate to send me a mail to <hendrik.erz@liu.se> or ping me on Twitter <https://twitter.com/sahiralsaid>!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP Workshop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
