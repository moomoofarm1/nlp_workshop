{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TlOmTfmIQPMK"
      },
      "source": [
        "# SICSS Norrk√∂ping 2023: Text Analysis\n",
        "\n",
        "_Hendrik Erz, Institute for Analytical Sociology | <hendrik.erz@liu.se> | Twitter: @sahiralsaid_\n",
        "\n",
        "Welcome to the practical part of the Text Analaysis day! In this notebook, you will try out some of the methods covered in the morning lecture. In particular, the methods covered will be:\n",
        "\n",
        "* tf-idf scores\n",
        "* topic modeling\n",
        "* Word2Vec\n",
        "\n",
        "Below, you will see several exercises that cover most of the steps from an unprocessed text corpus to a final trained model, and, lastly the analysis step.\n",
        "\n",
        "You will work on these examples in smaller groups with guidance from me."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8bOzIeukQxKZ"
      },
      "source": [
        "## Preliminaries: Loading the Corpus\n",
        "\n",
        "Your task in this exercise is to write a function that will return parts of the corpus in a way that the models we are using works with. Here, we will be working with a corpus of the **[States of the Union (SOTU)](https://en.wikipedia.org/wiki/State_of_the_Union) of the U.S. presidents**.\n",
        "\n",
        "***\n",
        "\n",
        "The first step is always to load the corpus. We will use a **generator** for this, since a generator helps us keep the memory footprint small and therefore to keep the model training times low.\n",
        "\n",
        "Below you can find a ready-made function that will automatically return the corpus in the following format:\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "    ('This is a speech from a republican', 'R'),\n",
        "    ('This is a speech from a democrat', 'D'),\n",
        "    # ...\n",
        "]\n",
        "```\n",
        "\n",
        "As you can see, you will get from this generator function a list of **tuples**. The first element is always a speech, the second element is a letter indicating the president's party. The party codes are as follows:\n",
        "\n",
        "* R: Republican\n",
        "* D: Democrat\n",
        "* W: Whig\n",
        "* F: Federalist\n",
        "* DR: Democratic-Republican\n",
        "* na: No party\n",
        "* NU: National Union\n",
        "\n",
        "***\n",
        "\n",
        "**Whenever you need the speeches, just call `speeches()` in your code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "aogeN5TsSNZR"
      },
      "outputs": [],
      "source": [
        "def speeches ():\n",
        "  \"\"\"A generator that yields (speech, party) tuples\"\"\"\n",
        "  # NOTE: Ensure that the file is in the same directory as this notebook.\n",
        "  with open(\"sotu.csv\", \"r\", encoding=\"utf-8\") as fp:\n",
        "    next(fp) # Skip the header\n",
        "    for line in fp:\n",
        "      speech, party = line.strip().split('\\t')\n",
        "      yield (speech, party)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ9cSO93VppW",
        "outputId": "bbfa1032-1d83-4ece-edf9-75d59a8f749e"
      },
      "outputs": [],
      "source": [
        "# Make sure we have all 251 speeches in our generator\n",
        "sum([1 for x in speeches()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries: Installing the necessary packages\n",
        "\n",
        "This workbook relies on a set of packages. Depending on what package manager you use, there are various ways to install them (e.g., with Conda, pip, or virtualenv).\n",
        "\n",
        "Make sure to install them according to how you have set up Python. If you use plain `pip`, here is how you can install them:\n",
        "\n",
        "```bash\n",
        "python -m pip install pandas\n",
        "python -m pip install numpy\n",
        "python -m pip install gensim\n",
        "python -m pip install sklearn\n",
        "python -m pip install nltk\n",
        "python -m pip install matplotlib\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting to Know the Corpus\n",
        "\n",
        "Before we dive into the corpus, we should make sure we have some bearings in the corpus. For this, a very simple method is to simply start counting words. This is not necessarily the most informative method, but it will give us some data that we can use to make some sense of the corpus.\n",
        "\n",
        "Getting to some word counts involves a few steps:\n",
        "\n",
        "1. First, we need a **tokenizer** that can split up a speech into tokens.\n",
        "2. After we have a method of splitting a document into its constitutent words, we can use this information to build a vocabulary.\n",
        "3. We can count words to ensure our corpus looks as it should."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9zLHzCvEZu"
      },
      "source": [
        "### Exercise 1: Write a text tokenizer\n",
        "\n",
        "Below, write a function that takes a single speech and returns a list of tokens. You should remove digits, punctuation marks, and other symbols that do not comprise regular, English words. Also, ensure that capitalization of letters does not matter by transforming the speech into lower case first.\n",
        "\n",
        "> TIP: The NLTK package offers a lot of useful functions for working with natural language. It includes functions to remove stopwords and to tokenize a text. Also, the String class of Python provides additional easy functions you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hmpNEFrJTbqk"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Loads a common english stopword list\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "def tokenize_speech(speech):\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYkyW1DfXna4",
        "outputId": "072e18e4-3aaa-4f68-fe9e-f04f4c802686"
      },
      "outputs": [],
      "source": [
        "# Run this cell to see how the preprocessor processes the first speech.\n",
        "first_speech = next(speeches())[0]\n",
        "tokenize_speech(first_speech)[0:10] # Look at the first ten words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVDE8UhMy_SY"
      },
      "source": [
        "### Exercise 2: Build a Vocabulary\n",
        "\n",
        "We will need a vocabulary for many of the following steps, so it makes sense to write a dedicated function for it. The easiest form of a vocabulary is a dictionary that maps words to indices:\n",
        "\n",
        "```python\n",
        "vocab = {\n",
        "    'word': 0,\n",
        "    'word2': 1,\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "Since we also need to figure out words by their indices after running some of our methods, we should create a so-called `i2w`-dictionary. The `i2w` (\"index to word\") performs the reverse lookup and maps indices to words:\n",
        "\n",
        "```python\n",
        "iw2 = {\n",
        "    0: 'word',\n",
        "    1: 'word2',\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "We can then use these dictionaries to retrieve indices or words, depending on what we need:\n",
        "\n",
        "```python\n",
        "vocab['president']\n",
        "# > May yield '24'\n",
        "\n",
        "word = i2w[24]\n",
        "# > May yield 'president'\n",
        "```\n",
        "\n",
        "Below, write a function that returns both a vocab and an i2w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cYkvFH6Fzlkd"
      },
      "outputs": [],
      "source": [
        "def build_vocab ():\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to create the vocabulary\n",
        "vocab, i2w = build_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let us have a first look at the first ten words in our vocabulary. Note how\n",
        "# the first ten words in our vocab equal the first words of the first speech.\n",
        "list(vocab.keys())[0:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Count words\n",
        "\n",
        "The final introductory exercise involves counting words. Python luckily has a useful function for us: `Counter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Below, write a function that can count the words and returns a list of words\n",
        "# and their frequencies.\n",
        "from collections import Counter\n",
        "\n",
        "def count_words ():\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let us now look at the ten most common, and then ten least common words:\n",
        "\n",
        "words = count_words()\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(words[0:10])\n",
        "\n",
        "print(\"\")\n",
        "print(\"Least common words:\")\n",
        "print(words[-10:])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Zipf's Law\n",
        "\n",
        "One great way of ensuring that your corpus does not include oddities and spurious data, a very simple verification involves checking for Zipf's law: The fact that a few words are very common, and many words are relatively uncommon. A histogram of words should resemple a logarithmic function.\n",
        "\n",
        "Below, plot the frequencies ordered by rank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract only the actual numbers, and sort them by size descending.\n",
        "word_counts = sorted(list(map(lambda x: x[1], words)), reverse=True)\n",
        "\n",
        "plt.plot(range(len(word_counts)), word_counts)\n",
        "plt.yscale(\"log\") # Set the Y-axis to a logarithmic scale.\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.title(\"Zipf's Law: State of the Union Corpus\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rk7RST298"
      },
      "source": [
        "## Computing tf-idf scores\n",
        "\n",
        "The most simple way to begin an analysis is by calculating tf-idf scores. You can do this \"manually\", but for practical usage, there are some libraries that already do that for you.\n",
        "\n",
        "Calculating tf-idf scores consists of three steps:\n",
        "\n",
        "1. Calculate the term frequencies of words within each document\n",
        "2. Calculate the inverse document frequency for each word\n",
        "3. For each word in each document, calculate tf-idf by multiplying the document-specific frequency with the word's IDF-score\n",
        "\n",
        "Remember, tf-idf is defined as:\n",
        "\n",
        "$$\n",
        "{\\displaystyle \\text{tf-idf} (t, d, D) = \\mathrm{tf} (t,d) \\times \\mathrm{idf}}(t, D)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}}\n",
        "$$\n",
        "\n",
        "with $t$ = the term in question, $t'$ = all other terms, and $f_t$ = the (document)relative frequency of the term. And:\n",
        "\n",
        "$$\n",
        " \\mathrm{idf}(t, D) =  \\log \\frac{N}{1 + D}\n",
        "$$\n",
        "\n",
        "with $N$ = total number of documents in the corpus and $D$ = number of documents that contain term $t$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaNUs1KeK9B"
      },
      "source": [
        "### Exercise 4: Calculate tf-idf scores\n",
        "\n",
        "Below, write a function that calculates tf-idf scores for our corpus and then outputs the most relevant word for each speech.\n",
        "\n",
        "> The library `scikit-learn` has a useful function for that called `TfidfVectorizer`. You will need to pass it the vocabulary and the speeches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "zlf364UDeaao"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tf_idf ():\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoJk1kg8f17T"
      },
      "outputs": [],
      "source": [
        "# Call the function and calculate the tf-idf scores\n",
        "tf_idf = calculate_tf_idf()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTqkGzUfWCf"
      },
      "source": [
        "### Exercise 5: Analysis of tf-idf scores\n",
        "\n",
        "Now that we have calculated tf-idf scores, it is time to have a look at them. Below, write code to print out the highest scoring word for each speech, as well as, afterwards, the lowest-scoring word.\n",
        "\n",
        "Explain what makes the words important or unimportant, and what this means in the context of the SOTU corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN09mxipgBBV",
        "outputId": "c46d5f89-99f8-44cf-f3f3-b5e048b3dac0"
      },
      "outputs": [],
      "source": [
        "print(\"Most important words (according to tf-idf):\")\n",
        "# TODO: Your code here\n",
        "\n",
        "print(\"\")\n",
        "print(\"Least important words (according to tf-idf):\")\n",
        "# TODO: Your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f4LDxhirgDjr"
      },
      "source": [
        "## Running a Topic Model\n",
        "\n",
        "The next step to see what is inside our corpus is to run a topic model. The most common model is Latent Dirichlet Allocation (LDA). The library `sklearn` already provides such a model. However, again, here we have to preprocess the sentences. However, this time, we have to do it differently.\n",
        "\n",
        "Running an LDA model requires a so-called Document-Term Matrix (DTM). In it, documents are defined as \"one hot\"-vectors. The matrix has the shape `(number of documents, number of words)`, and each cell is set to `0` if the document does not contain the word, and `1` if it does.\n",
        "\n",
        "With the `tokenize_speech()` function from above, we already have a function that splits speeches into tokens. We now just need to build the DTM based on that. Building a DTM consists normally of these steps:\n",
        "\n",
        "1. Create a vocabulary that contains every token within the whole corpus\n",
        "2. Optionally, remove the most often occurring and the least often occurring terms to reduce the amount of words\n",
        "3. Go over the corpus and set the corresponding cells in the matrix to `1`, if the document contains a word in the vocabulary."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fTtUrW9iyM6U"
      },
      "source": [
        "### Exercise 6: Build a DTM\n",
        "\n",
        "Below, write a function that creates a DTM. We have already provided a matrix that is set to all zeros and can be fed into the LDA function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "qXhoT2Jeh3FJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dtm ():\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iOxl2vJJikKL"
      },
      "source": [
        "### Exercise 7: Run the Topic model\n",
        "\n",
        "Below, write a function that trains a topic model. I have already added the correct function import for you. One thing that you will need to do, however, is figure out three hyperparameters: K, alpha, and beta.\n",
        "\n",
        "Since we are dealing with a small corpus, let us just set $K = 1$. However, you still need to figure out a good alpha and a good beta. Beta should normally be larger than alpha, and both should be smaller than 0.5. Feel free to run the model several times while doing exercise 6 to figure out good values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "m42pLJa7is6A"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def fit_lda_model ():\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "qvRyMb9Dus1R"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "model = fit_lda_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P9RRDWX5jPHU"
      },
      "source": [
        "### Exercise 8: Analyze the topic model\n",
        "\n",
        "The last step in this exercise is to analyse the topic model. The most common method is to simply output the most important words (here, let us use 10 words) for each topic and see if you can make out any semantic topics.\n",
        "\n",
        "> Below, write a function that prints the ten most important words for each topic. Try to think of fitting topic labels. What topic do the words seem to focus on? TIP: In order to sort the words correctly, you can use the functions `np.argsort` and, afterwards, `np.fliplr`, to reverse the order of the top words. Additionally, the topic-term-matrix is accessible with the property `components_` of the trained model. The shape of this matrix is `(n_topics, n_words)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3szwevOvj8Qy"
      },
      "outputs": [],
      "source": [
        "def print_top_words (model):\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZCTr_qCuJ6M",
        "outputId": "10f6456e-24f3-466b-f65d-ef25c7a32ed9"
      },
      "outputs": [],
      "source": [
        "# Call the function\n",
        "print_top_words(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2FqixBzziY"
      },
      "source": [
        "## Running Word2Vec\n",
        "\n",
        "The most advanced NLP method we will cover today is a Word2Vec model. Such a model encodes co-occurrence patterns of words in so-called word embeddings, vectors of numbers with 50, 100, 200, or 300 dimensions.\n",
        "\n",
        "Here, you will write the least code since we will be using the gensim-library to run Word2Vec. Despite it being the (mathematically) most complex model, it can be run very fast and efficiently."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wF-qkFnL0vOY"
      },
      "source": [
        "### Exercise 9: Run Word2Vec\n",
        "\n",
        "Below, write code that uses gensim's Word2Vec model and run it on our corpus.\n",
        "\n",
        "Train two models, one with a `window size` of 5, and one with 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "uF__0S4E0nYF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oN9wQDhC1mtr"
      },
      "source": [
        "### Exercise 10: Analyze the word embeddings\n",
        "\n",
        "As a last exercise for today, here we analyze the word embeddings. Word embeddings are very good to find out what words are related to others. On gensim's model, you can check so by utilizing `model.wv.most_similar('word')`.\n",
        "\n",
        "> Below, print out the most similar words for `america`, `government`, `bank`, and `war` for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLM9FcP715SG",
        "outputId": "93d2b349-6414-4c20-a671-c94effbec35a"
      },
      "outputs": [],
      "source": [
        "# TODO: Your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This concludes today's introduction into text analysis methods. Notice how we produced almost no plots: These methods are all great for getting a bearing on some corpus. Once you have found some interesting patterns, it is time to throw the results of one or more of the above methods into a model and test hypotheses with it.\n",
        "\n",
        "Regardless of whether you have used the provided corpus, or your own: Try to come up with some research questions over the afternoon that you could check. Maybe this leads to a great project idea?\n",
        "\n",
        "If you have further questions regarding these methods, do not hesitate to send me a mail to <hendrik.erz@liu.se> or ping me on Twitter <https://twitter.com/sahiralsaid>!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP Workshop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
