{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlOmTfmIQPMK"
      },
      "source": [
        "# NLP Workshop (SOLUTIONS)\n",
        "\n",
        "_Hendrik Erz, Institute for Analytical Sociology | <hendrik.erz@liu.se> | Twitter: @sahiralsaid_\n",
        "\n",
        "Welcome to the practical part of the NLP Workshop! In this notebook, you will try out some of the methods covered in the theoretical section. In particular, the methods covered will be:\n",
        "\n",
        "* tf-idf scores\n",
        "* topic modeling\n",
        "* Word2Vec\n",
        "\n",
        "Below, you will see several exercises that cover most of the steps from an unprocessed text corpus to a final trained model, and, lastly the analysis step.\n",
        "\n",
        "You will work on these examples in smaller groups with guidance from me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bOzIeukQxKZ"
      },
      "source": [
        "## Preliminaries: Loading the Corpus\n",
        "\n",
        "Your task in this exercise is to write a function that will return parts of the corpus in a way that the models we are using works with. Here, we will be working with a corpus of the **[States of the Union (SOTU)](https://en.wikipedia.org/wiki/State_of_the_Union) of the U.S. presidents**.\n",
        "\n",
        "***\n",
        "\n",
        "The first step is always to load the corpus. We will use a **generator** for this, since a generator helps us keep the memory footprint small and therefore to keep the model training times low.\n",
        "\n",
        "Normally, you would have the corpus downloaded to your computer, but since we're on a Google Colab, we'll have to retrieve it from the web first. Since I provide the corpus, below you can find a ready-made function that will automatically return the corpus in the following format:\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "    ('This is a speech from a republican', 'R'),\n",
        "    ('This is a speech from a democrat', 'D'),\n",
        "    # ...\n",
        "]\n",
        "```\n",
        "\n",
        "As you can see, you will get from this generator function a list of **tuples**. The first element is always a speech, the second element is a letter indicating the president's party. The party codes are as follows:\n",
        "\n",
        "* R: Republican\n",
        "* D: Democrat\n",
        "* W: Whig\n",
        "* F: Federalist\n",
        "* DR: Democratic-Republican\n",
        "* na: No party\n",
        "* NU: National Union\n",
        "\n",
        "***\n",
        "\n",
        "**Whenever you need the speeches, just call `speeches()` in your code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aogeN5TsSNZR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request as request\n",
        "import shutil\n",
        "\n",
        "def maybe_download_file():\n",
        "    \"\"\"This function downloads the corpus to the VM and saves it to sotu.csv\"\"\"\n",
        "    outfile = \"sotu.csv\"\n",
        "    # The file is about 12MB large and contains 251 speeches.\n",
        "    file_link = \"https://gist.githubusercontent.com/nathanlesage/241cecdbd9a2f97146784abdb063d566/raw/26c17e63889575900cf0140eadcb84056193c78e/sotu.csv\"\n",
        "    if not os.path.exists (outfile):\n",
        "        with request.urlopen(file_link) as response, open(outfile, 'wb') as fp:\n",
        "            shutil.copyfileobj(response, fp)\n",
        "\n",
        "def speeches ():\n",
        "    \"\"\"A generator that yields (speech, party) tuples\"\"\"\n",
        "    maybe_download_file()\n",
        "\n",
        "    with open(\"sotu.csv\", \"r\", encoding=\"utf-8\") as fp:\n",
        "        for line in fp:\n",
        "            speech, party = line.split('\\t')\n",
        "            yield (speech, party)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ9cSO93VppW",
        "outputId": "bbfa1032-1d83-4ece-edf9-75d59a8f749e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "251"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure we have all 251 speeches in our generator\n",
        "sum([1 for x in speeches()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rk7RST298"
      },
      "source": [
        "## Computing tf-idf scores\n",
        "\n",
        "The most simple way to begin an analysis is by calculating tf-idf scores. Here we will do this \"manually\" so that you get a sense for what this means. For practical usage, there are some libraries that already do that for you.\n",
        "\n",
        "Calculating tf-idf scores consists of two steps:\n",
        "\n",
        "1. Define a function that preprocesses the speeches and returns individual tokens\n",
        "2. Call that function, count the words and calculate the tf-idf scores.\n",
        "\n",
        "Remember, tf-idf is defined as:\n",
        "\n",
        "$$\n",
        "{\\displaystyle \\text{tf-idf} (t, d, D) = \\mathrm{tf} (t,d) \\times \\mathrm{idf}}(t, D)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}}\n",
        "$$\n",
        "\n",
        "with $t$ = the term in question, $t'$ = all other terms, and $f_t$ = the relative frequency of the term. And:\n",
        "\n",
        "$$\n",
        " \\mathrm{idf}(t, D) =  \\log \\frac{N}{1 + D}\n",
        "$$\n",
        "\n",
        "with $N$ = total number of documents in the corpus and $D$ = number of documents that contain term $t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9zLHzCvEZu"
      },
      "source": [
        "### Exercise 1: Preprocess the text for tf-idf\n",
        "\n",
        "Below, write a function that takes speeches as returned from the function above and returns a list of tokens. You should remove digits, punctuation marks, and other symbols that do not comprise regular, English words.\n",
        "\n",
        "> TIP: The NLTK package offers a lot of useful functions for working with natural language. It includes functions to remove so-called stopwords and to tokenize a text. Also, the String class of Python provides additional easy functions you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hmpNEFrJTbqk"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Loads a common english stopword list\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "def preprocess_speeches():\n",
        "    for speech, _ in speeches():\n",
        "        # The following first converts the speech to lowercase and then tokenizes\n",
        "        # it with NLTK's built-in tokenizer. It then also removes words that are\n",
        "        # not alphanumeric (i.e. punctuation and numbers) and stopwords.\n",
        "        yield [t for t in word_tokenize(speech.lower()) if t.isalpha() and t not in stops]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYkyW1DfXna4",
        "outputId": "072e18e4-3aaa-4f68-fe9e-f04f4c802686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['good',\n",
              " 'afternoon',\n",
              " 'beginning',\n",
              " 'new',\n",
              " 'year',\n",
              " 'reflect',\n",
              " 'state',\n",
              " 'american',\n",
              " 'union',\n",
              " 'seek',\n",
              " 'definition',\n",
              " 'america',\n",
              " 'means',\n",
              " 'carl',\n",
              " 'sandburg',\n",
              " 'came',\n",
              " 'close',\n",
              " 'capturing',\n",
              " 'real',\n",
              " 'meaning',\n",
              " 'three',\n",
              " 'simple',\n",
              " 'words',\n",
              " 'became',\n",
              " 'title',\n",
              " 'one',\n",
              " 'greatest',\n",
              " 'poems',\n",
              " 'people',\n",
              " 'yes',\n",
              " 'america',\n",
              " 'risen',\n",
              " 'greatness',\n",
              " 'chips',\n",
              " 'american',\n",
              " 'people',\n",
              " 'said',\n",
              " 'yes',\n",
              " 'yes',\n",
              " 'challenge',\n",
              " 'freedom',\n",
              " 'yes',\n",
              " 'dare',\n",
              " 'progress',\n",
              " 'yes',\n",
              " 'hope',\n",
              " 'peace',\n",
              " 'defending',\n",
              " 'peace',\n",
              " 'meant',\n",
              " 'paying',\n",
              " 'price',\n",
              " 'war',\n",
              " 'america',\n",
              " 'greatness',\n",
              " 'endure',\n",
              " 'future',\n",
              " 'institutions',\n",
              " 'continually',\n",
              " 'rededicate',\n",
              " 'saying',\n",
              " 'yes',\n",
              " 'people',\n",
              " 'yes',\n",
              " 'human',\n",
              " 'needs',\n",
              " 'aspirations',\n",
              " 'yes',\n",
              " 'democracy',\n",
              " 'consent',\n",
              " 'governed',\n",
              " 'yes',\n",
              " 'equal',\n",
              " 'opportunity',\n",
              " 'unlimited',\n",
              " 'horizons',\n",
              " 'achievement',\n",
              " 'every',\n",
              " 'american',\n",
              " 'spirit',\n",
              " 'rededication',\n",
              " 'send',\n",
              " 'congress',\n",
              " 'next',\n",
              " 'days',\n",
              " 'fourth',\n",
              " 'section',\n",
              " 'state',\n",
              " 'union',\n",
              " 'report',\n",
              " 'message',\n",
              " 'progress',\n",
              " 'made',\n",
              " 'steps',\n",
              " 'must',\n",
              " 'take',\n",
              " 'helping',\n",
              " 'people',\n",
              " 'help',\n",
              " 'federal',\n",
              " 'programs',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'nineteen',\n",
              " 'hundred',\n",
              " 'year',\n",
              " 'full',\n",
              " 'opportunity',\n",
              " 'great',\n",
              " 'advances',\n",
              " 'front',\n",
              " 'years',\n",
              " 'war',\n",
              " 'successfully',\n",
              " 'completed',\n",
              " 'one',\n",
              " 'unselfish',\n",
              " 'missions',\n",
              " 'ever',\n",
              " 'undertaken',\n",
              " 'one',\n",
              " 'nation',\n",
              " 'defense',\n",
              " 'another',\n",
              " 'coming',\n",
              " 'peace',\n",
              " 'permits',\n",
              " 'us',\n",
              " 'turn',\n",
              " 'attention',\n",
              " 'fully',\n",
              " 'works',\n",
              " 'compassion',\n",
              " 'concern',\n",
              " 'social',\n",
              " 'progress',\n",
              " 'home',\n",
              " 'seriousness',\n",
              " 'government',\n",
              " 'commitment',\n",
              " 'make',\n",
              " 'opportunity',\n",
              " 'evidenced',\n",
              " 'record',\n",
              " 'level',\n",
              " 'funding',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'programs',\n",
              " 'proposed',\n",
              " 'new',\n",
              " 'budget',\n",
              " 'twice',\n",
              " 'amount',\n",
              " 'spent',\n",
              " 'programs',\n",
              " 'took',\n",
              " 'office',\n",
              " 'let',\n",
              " 'us',\n",
              " 'look',\n",
              " 'behind',\n",
              " 'impersonal',\n",
              " 'label',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'let',\n",
              " 'us',\n",
              " 'see',\n",
              " 'examples',\n",
              " 'way',\n",
              " 'programs',\n",
              " 'helping',\n",
              " 'provide',\n",
              " 'better',\n",
              " 'life',\n",
              " 'american',\n",
              " 'people',\n",
              " 'social',\n",
              " 'security',\n",
              " 'cash',\n",
              " 'benefits',\n",
              " 'elderly',\n",
              " 'disabled',\n",
              " 'fiscal',\n",
              " 'year',\n",
              " 'twice',\n",
              " 'years',\n",
              " 'ago',\n",
              " 'next',\n",
              " 'year',\n",
              " 'million',\n",
              " 'additional',\n",
              " 'poor',\n",
              " 'aged',\n",
              " 'disabled',\n",
              " 'persons',\n",
              " 'receive',\n",
              " 'increased',\n",
              " 'health',\n",
              " 'benefits',\n",
              " 'hundreds',\n",
              " 'counties',\n",
              " 'previously',\n",
              " 'food',\n",
              " 'programs',\n",
              " 'assure',\n",
              " 'nutrition',\n",
              " 'needy',\n",
              " 'hunger',\n",
              " 'eliminated',\n",
              " 'american',\n",
              " 'life',\n",
              " 'hundreds',\n",
              " 'school',\n",
              " 'districts',\n",
              " 'giving',\n",
              " 'black',\n",
              " 'brown',\n",
              " 'children',\n",
              " 'inferior',\n",
              " 'educations',\n",
              " 'separate',\n",
              " 'school',\n",
              " 'systems',\n",
              " 'time',\n",
              " 'took',\n",
              " 'office',\n",
              " 'give',\n",
              " 'children',\n",
              " 'equal',\n",
              " 'chance',\n",
              " 'learn',\n",
              " 'together',\n",
              " 'schools',\n",
              " 'new',\n",
              " 'student',\n",
              " 'assistance',\n",
              " 'system',\n",
              " 'established',\n",
              " 'bring',\n",
              " 'higher',\n",
              " 'education',\n",
              " 'within',\n",
              " 'reach',\n",
              " 'every',\n",
              " 'qualified',\n",
              " 'student',\n",
              " 'america',\n",
              " 'launched',\n",
              " 'national',\n",
              " 'drive',\n",
              " 'conquest',\n",
              " 'cancer',\n",
              " 'advanced',\n",
              " 'workable',\n",
              " 'proposal',\n",
              " 'provide',\n",
              " 'comprehensive',\n",
              " 'health',\n",
              " 'insurance',\n",
              " 'every',\n",
              " 'american',\n",
              " 'family',\n",
              " 'health',\n",
              " 'education',\n",
              " 'benefits',\n",
              " 'veterans',\n",
              " 'substantially',\n",
              " 'increased',\n",
              " 'job',\n",
              " 'programs',\n",
              " 'decreased',\n",
              " 'unemployment',\n",
              " 'rate',\n",
              " 'among',\n",
              " 'veterans',\n",
              " 'almost',\n",
              " 'past',\n",
              " 'year',\n",
              " 'alone',\n",
              " 'sweeping',\n",
              " 'reforms',\n",
              " 'set',\n",
              " 'motion',\n",
              " 'assure',\n",
              " 'senior',\n",
              " 'citizens',\n",
              " 'quality',\n",
              " 'nursing',\n",
              " 'home',\n",
              " 'care',\n",
              " 'better',\n",
              " 'chance',\n",
              " 'live',\n",
              " 'dignity',\n",
              " 'homes',\n",
              " 'legislative',\n",
              " 'proposals',\n",
              " 'increase',\n",
              " 'economic',\n",
              " 'opportunity',\n",
              " 'american',\n",
              " 'indian',\n",
              " 'laid',\n",
              " 'congress',\n",
              " 'resubmitted',\n",
              " 'congress',\n",
              " 'year',\n",
              " 'outlays',\n",
              " 'civil',\n",
              " 'rights',\n",
              " 'activities',\n",
              " 'billion',\n",
              " 'times',\n",
              " 'beginning',\n",
              " 'administration',\n",
              " 'support',\n",
              " 'closer',\n",
              " 'today',\n",
              " 'ever',\n",
              " 'realization',\n",
              " 'truly',\n",
              " 'society',\n",
              " 'men',\n",
              " 'women',\n",
              " 'equal',\n",
              " 'eyes',\n",
              " 'law',\n",
              " 'achievements',\n",
              " 'others',\n",
              " 'outline',\n",
              " 'message',\n",
              " 'congress',\n",
              " 'constitute',\n",
              " 'record',\n",
              " 'proud',\n",
              " 'good',\n",
              " 'beginning',\n",
              " 'build',\n",
              " 'certain',\n",
              " 'aspects',\n",
              " 'state',\n",
              " 'union',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'urgently',\n",
              " 'need',\n",
              " 'reform',\n",
              " 'federal',\n",
              " 'government',\n",
              " 'undertook',\n",
              " 'ambitious',\n",
              " 'sometimes',\n",
              " 'almost',\n",
              " 'utopian',\n",
              " 'commitments',\n",
              " 'one',\n",
              " 'area',\n",
              " 'social',\n",
              " 'policy',\n",
              " 'another',\n",
              " 'elbowing',\n",
              " 'aside',\n",
              " 'state',\n",
              " 'local',\n",
              " 'governments',\n",
              " 'private',\n",
              " 'sector',\n",
              " 'establishing',\n",
              " 'literally',\n",
              " 'hundreds',\n",
              " 'new',\n",
              " 'programs',\n",
              " 'based',\n",
              " 'assumption',\n",
              " 'human',\n",
              " 'problem',\n",
              " 'could',\n",
              " 'solved',\n",
              " 'simply',\n",
              " 'throwing',\n",
              " 'enough',\n",
              " 'federal',\n",
              " 'dollars',\n",
              " 'intention',\n",
              " 'effort',\n",
              " 'laudable',\n",
              " 'results',\n",
              " 'case',\n",
              " 'case',\n",
              " 'amounted',\n",
              " 'dismal',\n",
              " 'failure',\n",
              " 'money',\n",
              " 'left',\n",
              " 'washington',\n",
              " 'seemingly',\n",
              " 'inexhaustible',\n",
              " 'flood',\n",
              " 'reduced',\n",
              " 'mere',\n",
              " 'trickle',\n",
              " 'time',\n",
              " 'filtered',\n",
              " 'layers',\n",
              " 'bureaucrats',\n",
              " 'consultants',\n",
              " 'social',\n",
              " 'workers',\n",
              " 'finally',\n",
              " 'reached',\n",
              " 'supposed',\n",
              " 'help',\n",
              " 'much',\n",
              " 'money',\n",
              " 'going',\n",
              " 'supposed',\n",
              " 'help',\n",
              " 'needy',\n",
              " 'little',\n",
              " 'needy',\n",
              " 'make',\n",
              " 'profession',\n",
              " 'poverty',\n",
              " 'got',\n",
              " 'fat',\n",
              " 'taxpayer',\n",
              " 'got',\n",
              " 'stuck',\n",
              " 'bill',\n",
              " 'disadvantaged',\n",
              " 'got',\n",
              " 'little',\n",
              " 'broken',\n",
              " 'promises',\n",
              " 'must',\n",
              " 'better',\n",
              " 'american',\n",
              " 'people',\n",
              " 'deserve',\n",
              " 'compassion',\n",
              " 'works',\n",
              " 'simply',\n",
              " 'compassion',\n",
              " 'means',\n",
              " 'well',\n",
              " 'deserve',\n",
              " 'programs',\n",
              " 'say',\n",
              " 'yes',\n",
              " 'human',\n",
              " 'needs',\n",
              " 'saying',\n",
              " 'paternalism',\n",
              " 'social',\n",
              " 'exploitation',\n",
              " 'waste',\n",
              " 'order',\n",
              " 'bring',\n",
              " 'programs',\n",
              " 'standard',\n",
              " 'carefully',\n",
              " 'reviewed',\n",
              " 'three',\n",
              " 'questions',\n",
              " 'mind',\n",
              " 'reform',\n",
              " 'decisionmaking',\n",
              " 'process',\n",
              " 'bring',\n",
              " 'closer',\n",
              " 'people',\n",
              " 'decisions',\n",
              " 'affect',\n",
              " 'get',\n",
              " 'value',\n",
              " 'productivity',\n",
              " 'every',\n",
              " 'tax',\n",
              " 'dollar',\n",
              " 'devoted',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'reform',\n",
              " 'approach',\n",
              " 'delivery',\n",
              " 'services',\n",
              " 'give',\n",
              " 'people',\n",
              " 'assistance',\n",
              " 'need',\n",
              " 'without',\n",
              " 'taking',\n",
              " 'away',\n",
              " 'freedom',\n",
              " 'decreasing',\n",
              " 'reforms',\n",
              " 'propose',\n",
              " 'give',\n",
              " 'people',\n",
              " 'served',\n",
              " 'better',\n",
              " 'greater',\n",
              " 'voice',\n",
              " 'education',\n",
              " 'manpower',\n",
              " 'training',\n",
              " 'programs',\n",
              " 'propose',\n",
              " 'convert',\n",
              " 'narrow',\n",
              " 'fragmented',\n",
              " 'categorical',\n",
              " 'programs',\n",
              " 'closely',\n",
              " 'controlled',\n",
              " 'washington',\n",
              " 'new',\n",
              " 'special',\n",
              " 'revenue',\n",
              " 'sharing',\n",
              " 'programs',\n",
              " 'provide',\n",
              " 'federal',\n",
              " 'funds',\n",
              " 'used',\n",
              " 'within',\n",
              " 'broad',\n",
              " 'areas',\n",
              " 'state',\n",
              " 'community',\n",
              " 'judges',\n",
              " 'best',\n",
              " 'meet',\n",
              " 'special',\n",
              " 'needs',\n",
              " 'make',\n",
              " 'federal',\n",
              " 'health',\n",
              " 'care',\n",
              " 'dollar',\n",
              " 'go',\n",
              " 'propose',\n",
              " 'eliminate',\n",
              " 'programs',\n",
              " 'whose',\n",
              " 'job',\n",
              " 'done',\n",
              " 'hospital',\n",
              " 'construction',\n",
              " 'subsidies',\n",
              " 'continued',\n",
              " 'would',\n",
              " 'worsen',\n",
              " 'national',\n",
              " 'oversupply',\n",
              " 'hospital',\n",
              " 'beds',\n",
              " 'inflate',\n",
              " 'medical',\n",
              " 'costs',\n",
              " 'savings',\n",
              " 'achieved',\n",
              " 'would',\n",
              " 'help',\n",
              " 'make',\n",
              " 'possible',\n",
              " 'increases',\n",
              " 'areas',\n",
              " 'million',\n",
              " 'next',\n",
              " 'year',\n",
              " 'cancer',\n",
              " 'heart',\n",
              " 'disease',\n",
              " 'research',\n",
              " 'make',\n",
              " 'economic',\n",
              " 'opportunity',\n",
              " 'dollar',\n",
              " 'go',\n",
              " 'propose',\n",
              " 'transfer',\n",
              " 'antipoverty',\n",
              " 'programs',\n",
              " 'conducted',\n",
              " 'office',\n",
              " 'economic',\n",
              " 'opportunity',\n",
              " 'appropriate',\n",
              " 'cabinet',\n",
              " 'departments',\n",
              " 'thereby',\n",
              " 'making',\n",
              " 'efficient',\n",
              " 'linking',\n",
              " 'related',\n",
              " 'federal',\n",
              " 'activities',\n",
              " 'ensure',\n",
              " 'people',\n",
              " 'provided',\n",
              " 'decent',\n",
              " 'income',\n",
              " 'circumstances',\n",
              " 'increase',\n",
              " 'human',\n",
              " 'dignity',\n",
              " 'rather',\n",
              " 'eroding',\n",
              " 'basic',\n",
              " 'values',\n",
              " 'family',\n",
              " 'structure',\n",
              " 'dignity',\n",
              " 'work',\n",
              " 'work',\n",
              " 'congress',\n",
              " 'improve',\n",
              " 'welfare',\n",
              " 'system',\n",
              " 'system',\n",
              " 'penalizes',\n",
              " 'person',\n",
              " 'going',\n",
              " 'work',\n",
              " 'rewards',\n",
              " 'person',\n",
              " 'going',\n",
              " 'welfare',\n",
              " 'totally',\n",
              " 'alien',\n",
              " 'american',\n",
              " 'tradition',\n",
              " 'reforming',\n",
              " 'present',\n",
              " 'welfare',\n",
              " 'system',\n",
              " 'continue',\n",
              " 'one',\n",
              " 'major',\n",
              " 'goals',\n",
              " 'overall',\n",
              " 'effect',\n",
              " 'reforms',\n",
              " 'elimination',\n",
              " 'programs',\n",
              " 'wasteful',\n",
              " 'concentrate',\n",
              " 'programs',\n",
              " 'work',\n",
              " 'make',\n",
              " 'possible',\n",
              " 'continued',\n",
              " 'growth',\n",
              " 'federal',\n",
              " 'efforts',\n",
              " 'meet',\n",
              " 'human',\n",
              " 'needs',\n",
              " 'time',\n",
              " 'helping',\n",
              " 'prevent',\n",
              " 'ballooning',\n",
              " 'budget',\n",
              " 'deficit',\n",
              " 'could',\n",
              " 'lead',\n",
              " 'higher',\n",
              " 'taxes',\n",
              " 'higher',\n",
              " 'prices',\n",
              " 'higher',\n",
              " 'interest',\n",
              " 'rates',\n",
              " 'americans',\n",
              " 'despite',\n",
              " 'people',\n",
              " 'say',\n",
              " 'fiscal',\n",
              " 'responsibility',\n",
              " 'rich',\n",
              " 'man',\n",
              " 'concern',\n",
              " 'spend',\n",
              " 'economy',\n",
              " 'tailspin',\n",
              " 'name',\n",
              " 'social',\n",
              " 'welfare',\n",
              " 'would',\n",
              " 'punishing',\n",
              " 'sought',\n",
              " 'help',\n",
              " 'course',\n",
              " 'history',\n",
              " 'free',\n",
              " 'american',\n",
              " 'economy',\n",
              " 'done',\n",
              " 'combat',\n",
              " 'poverty',\n",
              " 'raise',\n",
              " 'standard',\n",
              " 'living',\n",
              " 'government',\n",
              " 'program',\n",
              " 'imaginable',\n",
              " 'stable',\n",
              " 'healthy',\n",
              " 'growth',\n",
              " 'economy',\n",
              " 'must',\n",
              " 'remain',\n",
              " 'cornerstone',\n",
              " 'human',\n",
              " 'resources',\n",
              " 'policies',\n",
              " 'great',\n",
              " 'credit',\n",
              " 'americans',\n",
              " 'restless',\n",
              " 'impatient',\n",
              " 'people',\n",
              " 'nation',\n",
              " 'idealists',\n",
              " 'dream',\n",
              " 'eradicating',\n",
              " 'poverty',\n",
              " 'hunger',\n",
              " 'discrimination',\n",
              " 'ignorance',\n",
              " 'disease',\n",
              " 'fear',\n",
              " 'would',\n",
              " 'like',\n",
              " 'today',\n",
              " 'order',\n",
              " 'reach',\n",
              " 'goals',\n",
              " 'need',\n",
              " 'connect',\n",
              " 'warmhearted',\n",
              " 'impatience',\n",
              " 'another',\n",
              " 'equally',\n",
              " 'american',\n",
              " 'trait',\n",
              " 'levelheaded',\n",
              " 'common',\n",
              " 'sense',\n",
              " 'need',\n",
              " 'forge',\n",
              " 'new',\n",
              " 'approach',\n",
              " 'human',\n",
              " 'services',\n",
              " 'country',\n",
              " 'approach',\n",
              " 'treat',\n",
              " 'people',\n",
              " 'statistics',\n",
              " 'approach',\n",
              " 'recognizes',\n",
              " 'problems',\n",
              " 'like',\n",
              " 'poverty',\n",
              " 'unemployment',\n",
              " 'health',\n",
              " 'care',\n",
              " 'costs',\n",
              " 'education',\n",
              " 'cold',\n",
              " 'abstractions',\n",
              " 'government',\n",
              " 'file',\n",
              " 'drawer',\n",
              " 'know',\n",
              " 'tough',\n",
              " 'problems',\n",
              " 'grew',\n",
              " 'also',\n",
              " 'know',\n",
              " 'right',\n",
              " 'kind',\n",
              " 'help',\n",
              " 'right',\n",
              " 'kind',\n",
              " 'spirit',\n",
              " 'overcome',\n",
              " 'believe',\n",
              " 'american',\n",
              " 'family',\n",
              " 'denied',\n",
              " 'good',\n",
              " 'health',\n",
              " 'care',\n",
              " 'inability',\n",
              " 'pay',\n",
              " 'also',\n",
              " 'believe',\n",
              " 'family',\n",
              " 'deprived',\n",
              " 'freedom',\n",
              " 'make',\n",
              " 'health',\n",
              " 'care',\n",
              " 'arrangements',\n",
              " 'without',\n",
              " 'bureaucratic',\n",
              " 'meddling',\n",
              " 'believe',\n",
              " 'boy',\n",
              " 'girl',\n",
              " 'denied',\n",
              " 'quality',\n",
              " 'education',\n",
              " 'also',\n",
              " 'believe',\n",
              " 'child',\n",
              " 'ride',\n",
              " 'bus',\n",
              " 'miles',\n",
              " 'away',\n",
              " 'neighborhood',\n",
              " 'school',\n",
              " 'order',\n",
              " 'achieve',\n",
              " 'arbitrary',\n",
              " 'racial',\n",
              " 'balance',\n",
              " 'believe',\n",
              " 'american',\n",
              " 'family',\n",
              " 'suffer',\n",
              " 'lack',\n",
              " 'income',\n",
              " 'break',\n",
              " 'welfare',\n",
              " 'regulations',\n",
              " 'encourage',\n",
              " 'also',\n",
              " 'believe',\n",
              " 'never',\n",
              " 'make',\n",
              " 'comfortable',\n",
              " 'profitable',\n",
              " 'live',\n",
              " 'welfare',\n",
              " 'check',\n",
              " 'paycheck',\n",
              " 'believe',\n",
              " 'government',\n",
              " 'must',\n",
              " 'generous',\n",
              " 'humane',\n",
              " 'also',\n",
              " 'believe',\n",
              " 'government',\n",
              " 'must',\n",
              " 'economically',\n",
              " 'responsible',\n",
              " 'must',\n",
              " 'reform',\n",
              " 'end',\n",
              " 'programs',\n",
              " 'work',\n",
              " 'must',\n",
              " 'discontinue',\n",
              " 'programs',\n",
              " 'served',\n",
              " 'purpose',\n",
              " 'limited',\n",
              " 'resources',\n",
              " 'applied',\n",
              " 'programs',\n",
              " 'produce',\n",
              " 'cents',\n",
              " 'worth',\n",
              " 'human',\n",
              " 'benefits',\n",
              " 'every',\n",
              " 'tax',\n",
              " 'dollar',\n",
              " 'spent',\n",
              " 'working',\n",
              " 'together',\n",
              " 'meet',\n",
              " 'human',\n",
              " 'needs',\n",
              " 'unlock',\n",
              " 'human',\n",
              " 'potential',\n",
              " 'greatest',\n",
              " 'adventure',\n",
              " 'upon',\n",
              " 'people',\n",
              " 'embark',\n",
              " 'pledge',\n",
              " 'continued',\n",
              " 'strong',\n",
              " 'federal',\n",
              " 'leadership',\n",
              " 'work',\n",
              " 'learned',\n",
              " 'hard',\n",
              " 'way',\n",
              " 'washington',\n",
              " 'whole',\n",
              " 'job',\n",
              " 'state',\n",
              " 'local',\n",
              " 'governments',\n",
              " 'private',\n",
              " 'institutions',\n",
              " 'individual',\n",
              " 'american',\n",
              " 'must',\n",
              " 'part',\n",
              " 'well',\n",
              " 'let',\n",
              " 'us',\n",
              " 'give',\n",
              " 'citizens',\n",
              " 'help',\n",
              " 'need',\n",
              " 'let',\n",
              " 'us',\n",
              " 'also',\n",
              " 'remember',\n",
              " 'us',\n",
              " 'bears',\n",
              " 'basic',\n",
              " 'obligation',\n",
              " 'help',\n",
              " 'help',\n",
              " 'fellow',\n",
              " 'man',\n",
              " 'one',\n",
              " 'else',\n",
              " 'assume',\n",
              " 'obligation',\n",
              " 'us',\n",
              " 'least',\n",
              " 'federal',\n",
              " 'government',\n",
              " 'shirk',\n",
              " 'individual',\n",
              " 'responsibility',\n",
              " 'american',\n",
              " 'dream',\n",
              " 'never',\n",
              " 'dream',\n",
              " 'people',\n",
              " 'say',\n",
              " 'yes',\n",
              " 'challenge',\n",
              " 'government',\n",
              " 'says',\n",
              " 'yes',\n",
              " 'people',\n",
              " 'make',\n",
              " 'dream',\n",
              " 'come',\n",
              " 'true',\n",
              " 'lives',\n",
              " 'americans',\n",
              " 'thank',\n",
              " 'good',\n",
              " 'afternoon']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run this cell to see how the preprocessor processes the first speech.\n",
        "next(preprocess_speeches())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaNUs1KeK9B"
      },
      "source": [
        "### Exercise 2: Calculate tf-idf scores\n",
        "\n",
        "Below, write a function that takes the lists of tokens returned by the `preprocess_speeches()` function and returns a dictionary of a tf-idf scores for each word.\n",
        "\n",
        "> Remember that you will have to make several passes over the words, since you do not just need to calculate the relative frequencies of terms within a single document, but also which other documents contain a term. To index the documents, it suffices to use indices from 0 to the number of documents - 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zlf364UDeaao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_tf_idf ():\n",
        "    tfidf = dict()\n",
        "    # The dictionary should look like this:\n",
        "    # {\n",
        "    #   0: {\n",
        "    #     'word1': 0.4325,\n",
        "    #     'word2': 0.9512,\n",
        "    #     ...\n",
        "    #   },\n",
        "    #   1: {\n",
        "    #     'word2': 0.124,\n",
        "    #     ...\n",
        "    #   },\n",
        "    #  ...\n",
        "    # }\n",
        "\n",
        "    # Get the total number of documents\n",
        "    N = sum([1 for x in speeches()])\n",
        "\n",
        "    # Determine the number of documents in which each word occurs\n",
        "    doc_counter = Counter()\n",
        "    for tokens in preprocess_speeches():\n",
        "        doc_counter.update(set(tokens))\n",
        "\n",
        "    # Calculate the idf-scores for every word\n",
        "    idf = dict()\n",
        "    for word, count in doc_counter.items():\n",
        "        idf[word] = np.log(N / (1 + count))\n",
        "\n",
        "    # Now we can calculate tf-idf. We `enumerate` the speeches to have an index\n",
        "    # to refer to the documents, because each word has a different tf-idf score\n",
        "    # for every document\n",
        "    for idx, tokens in enumerate(preprocess_speeches()):\n",
        "        # Create the tf-idf dictionary\n",
        "        tfidf[idx] = dict()\n",
        "\n",
        "        # Count all the words in this specific document\n",
        "        local_frequency = Counter()\n",
        "        local_frequency.update(tokens)\n",
        "        local_sum = len(tokens) # Number of all words\n",
        "\n",
        "        # Calculate tf-idf\n",
        "        for token in tokens:\n",
        "            tf = local_frequency[token] / local_sum\n",
        "            tfidf[idx][token] = tf * idf[token]\n",
        "\n",
        "    return tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DoJk1kg8f17T"
      },
      "outputs": [],
      "source": [
        "# Call the function and calculate the tf-idf scores\n",
        "tfidf = calculate_tf_idf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTqkGzUfWCf"
      },
      "source": [
        "### Exercise 3: Analysis of tf-idf scores\n",
        "\n",
        "Below, write code to print out the highest scoring word for each speech, as well as, afterwards, the lowest-scoring word.\n",
        "\n",
        "Explain what makes the words important or unimportant, and what this means in the context of the SOTU corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN09mxipgBBV",
        "outputId": "c46d5f89-99f8-44cf-f3f3-b5e048b3dac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most important words (according to tf-idf)\n",
            "post, urban, colonies, tonight, nontaxable, california, heroin, submarines, ca, manila, constitution, complement, vietnam, hired, circuit, eighty, challenge, chickamaugas, texas, tell, gentlemen, applause, tile, attitudes, unrest, outline, percents, exhibition, croix, marijuana, seventies, warmest, nevada, chambers, axis, acquisition, missiles, balize, billion, freedmen, currency, cable, anarchist, florida, tons, german, disavowal, bank, hussein, poverty, slaves, objects, clarke, ports, fy, rebekah, contracts, violences, delawares, embargo, defeatism, cory, forest, economic, likewise, canal, alliance, regulatory, pennsylvania, augmentation, purchasing, bosnia, door, program, thank, afghanistan, banks, coinage, depression, hitler, tasks, iraqi, specifications, indemnity, observatory, savages, jobs, going, survey, removals, eighties, indians, percent, nitrates, isil, intimating, exclusions, cent, collective, smyrna, programs, spain, inflation, ny, gold, oil, negroes, un, silver, quick, privacy, attentions, evacuation, article, democracy, colonial, farmer, islands, isthmus, santo, thru, examinations, pension, seeds, enemy, yes, ministry, salt, autocracy, communist, iraq, technology, steamship, covenant, acres, soviet, wool, nitrogen, method, deforestation, nurses, livelihood, dictators, santiago, exchequer, mcleod, provinces, greytown, app, saddam, nominees, merged, strategic, licentiousness, megan, steven, lands, kansas, blockades, decrees, domingo, slavery, democratic, missile, businesses, reconversion, hull, ought, revisal, beauty, controls, environmental, emancipation, barrundia, kids, damages, interstate, enumerations, league, housing, harbors, natchez, subscriptions, discriminating, restart, chargé, recovery, steel, counties, kossuth, spending, laos, belligerency, mexico, notes\n",
            "\n",
            "Least important words (according to tf-idf)\n",
            "states, congress, government\n"
          ]
        }
      ],
      "source": [
        "# tfidf = calculate_tf_idf()\n",
        "\n",
        "print(\"Most important words (according to tf-idf)\")\n",
        "words = []\n",
        "for d in tfidf.values():\n",
        "  max_score = max(d.values())\n",
        "  [words.append(k) for k, tf in d.items() if tf == max_score]\n",
        "print(\", \".join(set(words)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Least important words (according to tf-idf)\")\n",
        "words = []\n",
        "for d in tfidf.values():\n",
        "  min_score = min(d.values())\n",
        "  [words.append(k) for k, tf in d.items() if tf == min_score]\n",
        "# Do you see how, by calling set() we see that we only have three least important words?\n",
        "print(\", \".join(set(words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4LDxhirgDjr"
      },
      "source": [
        "## Running a Topic Model\n",
        "\n",
        "The next step to see what is inside our corpus is to run a topic model. The most common model is Latent Dirichlet Allocation (LDA). The library `sklearn` already provides such a model. However, again, here we have to preprocess the sentences. However, this time, we have to do it differently.\n",
        "\n",
        "Running an LDA model requires a so-called Document-Term Matrix (DTM). In it, documents are defined as \"one hot\"-vectors. The matrix has the shape `(number of documents, number of words)`, and each cell is set to `0` if the document does not contain the word, and `1` if it does.\n",
        "\n",
        "With the `preprocess_speeches()` from above, we already have a function that spits out our tokens. We now just need to build the DTM based on that. Building a DTM consists normally of these steps:\n",
        "\n",
        "1. Create a vocabulary that contains every token within the whole corpus\n",
        "2. Optionally, remove the most often occurring and the least often occurring terms to reduce the amount of words\n",
        "3. Go over the corpus and set the corresponding cells in the matrix to `1`, if the document contains a word in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVDE8UhMy_SY"
      },
      "source": [
        "### Exercise 4: Build a Vocabulary\n",
        "\n",
        "We will need the vocabulary several times, so it makes sense to write a dedicated function for it. The easiest form of a vocabulary is a dictionary that maps words to indices:\n",
        "\n",
        "```python\n",
        "vocab = {\n",
        "    'word': 0,\n",
        "    'word2': 1,\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "Since we also need to figure out words by their indices, we should also create a so-called `i2w`-dictionary. The `i2w` performs the reverse lookup and maps indices to words:\n",
        "\n",
        "```python\n",
        "iw2 = {\n",
        "    0: 'word',\n",
        "    1: 'word2',\n",
        "    # ...\n",
        "}\n",
        "```\n",
        "\n",
        "Below, write a function that returns both a vocab and an i2w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cYkvFH6Fzlkd"
      },
      "outputs": [],
      "source": [
        "def build_vocab ():\n",
        "    # First generate the word -> index mapping\n",
        "    vocab = {}\n",
        "    for tokens in preprocess_speeches():\n",
        "        for token in tokens:\n",
        "            if not token in vocab:\n",
        "                # This basically adds unseen words to the end of the vocabulary\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "    # Now reverse it (important later), i.e. index -> word mapping.\n",
        "    # i2w = index2word\n",
        "    i2w = {}\n",
        "    for token in vocab:\n",
        "        i2w[vocab[token]] = token\n",
        "\n",
        "    return vocab, i2w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTtUrW9iyM6U"
      },
      "source": [
        "### Exercise 5: Build a DTM\n",
        "\n",
        "Below, write a function that creates a DTM. We have already provided a matrix that is set to all zeros and can be fed into the LDA function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qXhoT2Jeh3FJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dtm ():\n",
        "    # Retrieve the vocabulary\n",
        "    vocab, i2w = build_vocab()\n",
        "\n",
        "    # Instantiate the DTM with all zeros, meaning: At first, no document contains\n",
        "    # any word.\n",
        "    n_documents = sum([1 for x in speeches()])\n",
        "    n_words = len(vocab)\n",
        "    dtm = np.zeros((n_documents, n_words), dtype=np.longlong)\n",
        "\n",
        "    # Then, iterate over every document and every word, and set those cells to\n",
        "    # 1 where a word is contained in a document.\n",
        "    for idx, tokens in enumerate(preprocess_speeches()):\n",
        "        for token in tokens:\n",
        "            if token in vocab:\n",
        "                dtm[idx][vocab[token]] = 1\n",
        "\n",
        "    return dtm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOxl2vJJikKL"
      },
      "source": [
        "### Exercise 6: Run the Topic model\n",
        "\n",
        "Below, write a function that trains a topic model. I have already added the correct function import for you. One thing that you will need to do, however, is figure out three hyperparameters: K, alpha, and beta.\n",
        "\n",
        "Since we are dealing with a small corpus, let us just set $K = 1$. However, you still need to figure out a good alpha and a good beta. Beta should normally be larger than alpha, and both should be smaller than 0.5. Feel free to run the model several times while doing exercise 6 to figure out good values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m42pLJa7is6A"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def fit_lda_model ():\n",
        "    # First, retrieve the DTM and present the hyperparameters for the model.\n",
        "    dtm = build_dtm()\n",
        "    K = 10\n",
        "    alpha = 0.001\n",
        "    beta = 0.01\n",
        "\n",
        "    # Then instantiate the model, and fit it to our data\n",
        "    model = LatentDirichletAllocation(\n",
        "        n_components=K,\n",
        "        doc_topic_prior=alpha,\n",
        "        topic_word_prior=beta\n",
        "    )\n",
        "    model.fit(dtm)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qvRyMb9Dus1R"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "model = fit_lda_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9RRDWX5jPHU"
      },
      "source": [
        "### Exercise 7: Analyze the topic model\n",
        "\n",
        "The last step in this exercise is to analyse the topic model. The most common method is to simply output the most important words (here, let us use 10 words) for each topic and see if you can make out any semantic topics.\n",
        "\n",
        "> Below, write a function that prints the ten most important words for each topic. TIP: In order to sort the words correctly, you can use the functions `np.argsort` and, afterwards, `np.fliplr`, to reverse the order of the top words. Additionally, the topic-term-matrix is accessible with the property `components_` of the trained model. The shape of this matrix is `(n_topics, n_words)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3szwevOvj8Qy"
      },
      "outputs": [],
      "source": [
        "def print_top_words (model):\n",
        "    # Since we don't want to output numbers, we need an index->word mapping\n",
        "    _, i2w = build_vocab()\n",
        "\n",
        "    # How many words do we want to output?\n",
        "    L = 10\n",
        "\n",
        "    # This line first sorts every row ascending, i.e. the least important words\n",
        "    # for each topic are at the beginning, the most important words at the end.\n",
        "    # We then flip every row around so that the most important words are at the\n",
        "    # beginning. Lastly, we keep only the first ten words.\n",
        "    topic_list = np.fliplr(np.argsort(model.components_, axis=1))[:, :L] # Sort each row\n",
        "    for idx, words in enumerate(topic_list):\n",
        "        # Here we transform the indices from the topic_list to the actual words\n",
        "        w = [i2w[wd] for wd in words]\n",
        "        # Then print it as a comma-separated list\n",
        "        print(f\"Topic {idx + 1}: \" + \", \".join(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZCTr_qCuJ6M",
        "outputId": "10f6456e-24f3-466b-f65d-ef25c7a32ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 1: representatives, session, citizens, war, made, general, government, country, united, states\n",
            "Topic 2: imprudence, unabating, uncommon, fore, reenlisted, guthrie, certifications, barrow, encamping, hering\n",
            "Topic 3: policies, projects, activities, economic, problem, areas, problems, assistance, needs, needed\n",
            "Topic 4: consideration, commerce, duty, navy, might, representatives, due, opinion, treaty, within\n",
            "Topic 5: challenge, belief, achievements, construction, proposal, alone, proposals, half, general, governments\n",
            "Topic 6: economic, today, program, help, need, come, must, great, people, peace\n",
            "Topic 7: concentration, giving, putting, readily, individual, meeting, regard, extending, lines, respect\n",
            "Topic 8: help, sure, allies, ensure, pay, thank, defend, million, said, want\n",
            "Topic 9: thank, bless, tonight, laughter, ca, funding, reform, troops, medicare, nuclear\n",
            "Topic 10: absolutely, germany, otherwise, stands, ordered, judgment, seeking, seat, detention, perfectly\n"
          ]
        }
      ],
      "source": [
        "# Call the function\n",
        "print_top_words(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2FqixBzziY"
      },
      "source": [
        "### Running Word2Vec\n",
        "\n",
        "The most advanced NLP method we will cover today is a Word2Vec model. Such a model encodes co-occurrence patterns of words in so-called word embeddings, vectors of numbers with 50, 100, 200, or 300 dimensions.\n",
        "\n",
        "Here, you will write the least code since we will be using the gensim-library to run Word2Vec. However, due to requirements of the Word2Vec algorithms, we need to write a simple class that the Word2Vec model can use. Since engineering is not part of this workshop, I have provided this class already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WNoCVn730bFp"
      },
      "outputs": [],
      "source": [
        "class RestartableGenerator:\n",
        "    def __init__ (self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def __iter__ (self):\n",
        "        return self.func()\n",
        "\n",
        "# Create a new instance of this class by calling RestartableGenerator(preprocess_speeches) and pass that to Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF-qkFnL0vOY"
      },
      "source": [
        "### Exercise 8: Run Word2Vec\n",
        "\n",
        "Below, write code that imports gensim's Word2Vec model and run it on our corpus, utilizing the `RestartableGenerator` class so that Word2Vec can work with your preprocessing generator.\n",
        "\n",
        "Train two models, one with a `window size` of 5, and one with 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uF__0S4E0nYF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "gen = RestartableGenerator(preprocess_speeches)\n",
        "\n",
        "# The most complex model, but the least required code!\n",
        "w2v = Word2Vec(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN9wQDhC1mtr"
      },
      "source": [
        "### Exercise 9: Analyze the word embeddings\n",
        "\n",
        "As a last exercise for today, here we analyze the word embeddings. Word embeddings are very good to find out what words are related to others. On gensim's model, you can check so by utilizing `model.wv.most_similar('word')`.\n",
        "\n",
        "> Below, print out the most similar words for `america`, `government`, `bank`, and `war` for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLM9FcP715SG",
        "outputId": "93d2b349-6414-4c20-a671-c94effbec35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most similar words to 'america'\n",
            "[('allies', 0.8926410675048828), ('iraq', 0.8665159940719604), ('friends', 0.8628460168838501), ('democracy', 0.8561817407608032), ('proud', 0.8558210134506226), ('leadership', 0.8535337448120117), ('historic', 0.8531145453453064), ('iran', 0.8491712808609009), ('afghanistan', 0.8468578457832336), ('today', 0.8390277624130249)]\n",
            "\n",
            "Most similar words to 'government'\n",
            "[('courts', 0.6342569589614868), ('constitution', 0.600544810295105), ('authorities', 0.5990092754364014), ('form', 0.5988146066665649), ('jurisdiction', 0.5941332578659058), ('functions', 0.5925008058547974), ('judiciary', 0.5816866755485535), ('authority', 0.5756994485855103), ('legislature', 0.5613027811050415), ('executive', 0.557809591293335)]\n",
            "\n",
            "Most similar words to 'bank'\n",
            "[('banks', 0.8765361309051514), ('deposit', 0.8706388473510742), ('notes', 0.8547083735466003), ('circulating', 0.8458355069160461), ('deposits', 0.8433732986450195), ('redemption', 0.8371770977973938), ('circulation', 0.8337893486022949), ('bonds', 0.8309922814369202), ('redeemed', 0.8250311613082886), ('greenbacks', 0.8236985206604004)]\n",
            "\n",
            "Most similar words to 'war'\n",
            "[('troops', 0.693565309047699), ('navy', 0.6782442331314087), ('force', 0.6599565744400024), ('ii', 0.6520627737045288), ('forces', 0.6516596078872681), ('armed', 0.623936116695404), ('enemy', 0.6230311989784241), ('ships', 0.6197699308395386), ('army', 0.6102399826049805), ('skeleton', 0.5956263542175293)]\n"
          ]
        }
      ],
      "source": [
        "print(\"Most similar words to 'america'\")\n",
        "print(w2v.wv.most_similar('america'))\n",
        "print(\"\")\n",
        "print(\"Most similar words to 'government'\")\n",
        "print(w2v.wv.most_similar('government'))\n",
        "print(\"\")\n",
        "print(\"Most similar words to 'bank'\")\n",
        "print(w2v.wv.most_similar('bank'))\n",
        "print(\"\")\n",
        "print(\"Most similar words to 'war'\")\n",
        "print(w2v.wv.most_similar('war'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RteubzdvzWzH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP Workshop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
